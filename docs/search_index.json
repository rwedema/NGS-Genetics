[["index.html", "Case background information and exercises About Usage Learning outcomes End Products Rubric Schedule and deliverables", " Case background information and exercises Marcel Kempenaar and Ronald Wedema 2024-09-03 About This is the syllabus for the module 2.1.1 NGS and Genetics. This bookdown document will provide you with enough background material and exercises to complete the Case part of the module. Also links to the genetics presentations given can be found in the schedule here. Usage Chapters are divided into subjects. Each subject (chapter) should last you a week to complete. There is no new information after week 4 and the rest of the weeks should be spend on finishing the analysis of your selected case and preparing for the genetics exam. Learning outcomes You have a good understanding of all the concepts of the central dogma, different types of macromolecules, and enzymes, and you can describe their interconnections. You can identify various mechanisms of gene regulation and predict the effects of different mutations on gene expression. You are familiar with the different genome structures and can detect, filter, and annotate them in an NGS (Next-Generation Sequencing) dataset using appropriate tools and techniques. You have a clear understanding of the quality requirements for NGS data, depending on both the NGS technique used and the objective. You can also apply tools to assess and improve the quality of the data. You are aware of the available tools and algorithms for mapping NGS reads to a reference genome, and you can justify which one is best suited for a given dataset and question. You can provide arguments and present the choices made during the process of achieving a high-quality mapping. End Products The end products of this course are the following two items: The genetics exam (weight 3/5) Analysis of the selected case (weight 2/5) The two items above are weighted and combined to get a single final grade. See also the Rubric for the learning outcomes and how they are graded. Rubric Subject criteria weight Genetics exam The exam has been concluded with a passing grade. 15 Case The student writes a plan of approach and utilizes proper reasoning to justify the selection of the chosen tools. 3 Case The presentation provides a clear overview of the steps taken, and the final results are expressed in a clear and accurate manner. 4 Case Questions regarding the presentation and the obtained results of the worked-out case are answered clearly and in a structured manner. 2 Case During the lessons and in response to presentations by others, the student asks meaningful, focused, and clear questions, and also contributes additional information to the answers provided by others. 1 Schedule and deliverables Week Subject Case Chapter Background reading (Genetics) Note 1 Central Dogma en NGS techniques Chapter 1 see Blackboard 2 Gene regulation Chapter 2 see Blackboard 3 Mutations and effects thereof Chapter 3 see Blackboard 4 Finalize case Chapter 4 5 Finalize case Chapter 5 6 Finalize case The repair of the case is the bufferweek of quarter 2 7 Presentation case 8 Genetics exam 9 "],["intro.html", "Week 1 Intro NGS and Genetics Project 1.1 Cardiomyopathy 1.2 Medulloblastoma 1.3 Tools 1.4 Data 1.5 Analysis 1.6 Literature 1.7 Reporting", " Week 1 Intro NGS and Genetics Project In this project you will learn the bioinformatic basics of how to analyse Next-Generation Sequence (NGS) data of patients diagnosed with either cardiomyopathy or Medulloblastoma. Using this case we will discover genetic variations present in the genomic data of a patient having a suspected condition. The end-goal is to identify and report on those genomic variations that are possibly disease causing. All course material can be found in this document, some theory, a lot of links to resources, questions and assignments. 1.1 Cardiomyopathy As with many diseases, one of the causes of cardiomyopathy are a combination of genetic mutation(s). According to Wikipedia, the following forms of cardiomyopaty have a genetic base: Hypertrophic cardiomyopathy Arrhythmogenic right ventricular cardiomyopathy (ARVC) LV non-compaction Ion Channelopathies Dilated cardiomyopathy (DCM) Restrictive cardiomyopathy (RCM) The human genetics department of the University Medical Center Groningen diagnoses patients suspected to suffer from cardiomyopathy. As part of the diagnosis, the patients genome is compared to a set of reference genes known to be involved in the disease (called a gene panel). If variations are found they are checked and compared to known variants to classify or score their severity. Using these variations the type (dilated, restrictive, etc.) of the disease and how severe it is can be diagnosed, combined with regular data sources (physical exam, EKG, etc.). 1.2 Medulloblastoma Medulloblastoma is a type of brain cancer that occurs most often in children and is also the most common type of cancer in children. The cancer starts in the lower back part of the brain, called the cerebellum. The cerebellum is involved in muscle coordination, balance, and movement. It is a type of embryonal tumor, which means it starts in the fetal (embryonic) cells in the brain. The cancer cells are immature and often look like embryonic cells under a microscope. The cancer can spread to other areas of the brain and spinal cord. The cause of medulloblastoma is not known. It is not caused by head injury or exposure to radiation. It is not passed down through families. The cancer is more common in children with a weakened immune system and certain genetic problems, such as Gorlin syndrome or Turcot syndrome. The article accompanying the available data set specifies that there are multiple subtypes. This project aims to identify the subtypes of the available samples. “Traditionally, MB has been stratified into four histopathological subtypes based on histological appearance: Classic, Large Cell/Anaplastic, Desmoplastic/Nodular (DNMB) and MB with Extensive Nodularity (MBEN)3,4. Additionally, in the last decade, four molecular groups (WNT, SHH, Group 3, and Group 4) together with various subgroups have been defined and now generally replace the classic histopathological stratification in the 2021 WHO classification of central nervous system (CNS) tumors” Article: “Compartments in medulloblastoma with extensive nodularity are connected through differentiation along the granular precursor lineage”; https://www.nature.com/articles/s41467-023-44117-x Data set: https://www.ncbi.nlm.nih.gov/biosample?LinkName=bioproject_biosample_all&amp;from_uid=1044021 Data acquisition: “Libraries were enriched by hybrid capture with custom biotinylated RNA oligo pools covering exons of 130 cancer-associated genes. Paired-end sequencing was performed using the NextSeq 500 (Illumina).” Gene panels (all listed genes are included in the BED-file available in Galaxy): https://www.ncbi.nlm.nih.gov/gtr/tests/607978/ https://www.preventiongenetics.com/testInfo?val=Medulloblastoma-Panel https://repositorium.sdum.uminho.pt/bitstream/1822/57968/1/Leal_et_al-2018-Neuropathology.pdf Note that the article specifies 130 cancer-associated genes but does not refer to this list. The links above add up to 122 genes in total for genes related to (multiple types of) cancer. Subsequent data analysis will have to be performed to identify the genes that are actually captured for the exome sequencing. 1.3 Tools In this project we will work with many different software tools to replicate the genetic diagnoses process, both available either for download or on the computer you are now using. Other tools we are going to create ourselves! All of these tools however perform important steps in the analysis process and involve: checking the quality of input data, mapping the data to a reference genome (comparing with ‘known’ data), finding variations in respect to the reference and scoring the found variations on the probabilty that they are disease causing. Normally these steps involve many seperate tools which need to be run on the commandline, however for this course we will be using a worflow manager in which most of the tools are available and can be joined together to form logical steps in the analysis process. The workflow manager used in this course is Galaxy (wikipedia, website), but other worflow managers exist such as: CLC Bio, Taverna, Nextgene or SnakeMake. It is very likely that you will encounter one of these workflow managers in your future professional carreer, as many scientific laboratories do their biomedical research with the help of tools organised in such workflow managers. Next to the advantage of coupling multiple tools together into a workflow, Galaxy is the ideal translation from often hard-to-use commandline tools to easy-to-use by a large audience by offering simple graphical interfaces. Actually one of the reasons that you are following this course is to become proficient in also using these commandline tools and combining them into a workflow (also called a pipeline) so that non-technical researchers can use them! Week 1 of this course starts by explaining what Galaxy is and how you can use it to analyze your data, but first we will introduce and discuss the data we will be working with. 1.4 Data As a bioinformatician we often do not create the data we analyze ourselves but these come from a lab which - for this project - has a sequencer. This sequencer (an Illumina Miseq or NextSeq youtube, many other sequencing technologies exist and you will learn about these in a seperate lecture) generates sequencing data from a biological sample. Sequencing data for the cases introduced in this module will come from genome data. In a different module (2.1.2), you will also learn to work with transcriptomics data (RNA). The first step in performing a so called sequencing run is the sample-preparation. For this project this fase is used to filter the isolated DNA so that only the exons of the genes of interest (consisting of one or more exons or EXpressed regiONs) are kept, this method is called exome sequencing. By targeting specific genes (called a gene panel) we omit the bulk of the genome, and this has an important consequence. The human genome contains 26.564 genes that have 233.785 exons combined (when looking at the human genome build 2003). We can get more data on selectively targeting genes of interest, instead of trying to read the whole exome. Can you think of a reason that this is true? All DNA not included in the genes of interest and thus also the introns (somewhere around 98% of all DNA) is not sequenced, therefore from the total of ~3.2 billion basepairs only a small fraction is actually sequenced. This greatly depends on the case you selected and the number of genes present in the gene panel, but will typically be a very small percentage usually less than 1% of the genome. The actual data that we will be using is stored in relatively simple text-files containing the sequenced letters (A, C, G and T) along with some data primarily used to describe the quality of each sequenced base. Given the sequencing technology used to generate the sequencing data, we are limited to the machine’s technical capabilities to generate such data. In case of the MiSeq or NextSeq, this means that - unfortunately - not very long continuous sequences can be produced. The files generated by such sequencers contain millions of short sequences (~150 basepairs each, called a sequence read) with no particular order. The challenge with using this data to answer our initial question (which specific variations (mutations) are responsible for acquiring this disease) is to find out where each of these sequences originate from so that we can compare the patients’ sequence to the sequence of a healthy individual. With this comparison we can find if and where any variation is and thus begin with answering our question. 1.5 Analysis This course consists of a number of chapters like this that can be worked through in-order. The first section of each of these documents shows where in the analysis you are and which steps are next. It also shows what tools you are going to use and if there are any assignments included. You will note that this document for the first week is not very long nor does it contain too many assignments. This is on purpose since the goal of this week is primarily for you to understand: what the goal of this course is what is the disease you are looking into? what question(s) do we want to answer? the tools we will be using what is this Galaxy website? follow a tutorial to get familiar with it what kind of tools are available in Galaxy? the data which we will use throughout this course how is this data generated? which analysis steps are needed to answer our research question? Note on the assignments: If you are good at scanning documents you can easily spot the actual assignments in the first chapter and complete them in under two hours (note; this is not a challenge!). However, if I were to ask you to answer or explain some of the above questions you will probably have a hard time. To summarize, make sure that the general theory of what is shown here is clear at the end of this chapter. Either follow all the links to external resources, use google or (and this works pretty well) search for some of the terms or techniques on YouTube. Without this knowledge, you will manage to follow the steps during the first few weeks but will surely struggle later on when you need to make decisions on your own. Your final grade is not based on how you complete your assignments, but also on the level of understanding that you show in your lab journal and during the presentation. We begin the practical of each new week by first having a group discussing on what we did in the previous week. 1.6 Literature There is no explicit book for the case or other text that you will need to read, but there is a lot of online material available to use when you encounter unknown terms or concepts. The article titled ‘Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing’ discusses the complete process of analyzing raw sequencing data for variant analysis and while it goes beyond the scope of this project it is a good read. Besides this practical, the lectures from the Theory of Bioinformatics module also greatly add to your understanding of both the techniques, data and challenges we need to solve. 1.7 Reporting To help you work trough these document and prepare you as best as possible for the final presentation it is best to keep track of all your analysis steps and findings. As you will already have learned during the “Wetenschappelijke Cyclus” module, a type of document called R Markdown is very suitable for this purpose. In this document you can write a combination of text and code where the results (be it tables or figures) are included as well. Therefore, it is very suitable as a lab-journal for this course. There are no imposed rules on how to report your progress since it is not comparable with a normal written report (no chapters called ‘introduction’, ‘results’, etc.), just try to keep it organized, clearly state what you are doing and why this is important and where your results belong to. Do note however that when you use figures and tables (please do!), provide them with a clear caption to explain what you are showing. The logfile is not part of the grading (in the next module it will be!), here it is just a convenient way to keep track of all steps and results. 1.7.1 Galaxy Server This is the main ‘tool’ (or (web)platform) containing the actual tools for performing our analysis we will use during this course. Since we are spending a lot of time in Galaxy we start by learning how to use it. Note that there are a lot of Galaxy servers available to use worldwide and each server probably contains a different set of implemented tools. Typically, different labs have their own Galaxy server and host just the tool that are important for that particular lab to run their analyses. The main galaxy server is available at https://usegalaxy.org which is fine for learning about galaxy (section below) but won’t be used later on (see the section below that) and you do not have to register on that server. Learning Galaxy Start by following a beginner tutorial available at the Galaxy Training collection or from the - old - Galaxy Learn wiki page. First, browse the page to see what is available (some can be used later on too) and choose an interesting (beginners!) tutorial to try. While not all tutorials are either relavant or very up-to-date, you need to learn at least the following concepts: Creating a new history or rename your current - empty - history Uploading data from a local file Finding/ selecting a tool Selecting the input data for a tool changing settings for a tool Executing a tool Using the History to View tool-output Re-run a tool Delete elements Select and copy elements to a new history And - preferably - something about creating a workflow This item will be explained later on too You only need to report which tutorial you followed/ video tutorial you watched. Bioinformatics Galaxy Server After completing this tutorial, we switch to the Galaxy server that we will use throughout this course which is available at: https://galaxy.bioinf.nl and contains a collection of course specific tools. The Galaxy server that we host ourselves is different from the one that you’ve used for the tutorial. Different in this sense means containing a different collection of available software tools. Most of the tools that we will be using in this course can be found using the search tools input field, which can be found at the top left corner of Galaxy. Some tools however were installed in bundles and have their own category and therefore the easiest method of finding the tool you need is to simply enter the name in the search tools field in the top-left corner. 1.7.2 Data and quality control For this project we have patient data from patients suspectedly suffering from either Cardiomyopathy or Medulloblastoma you will be investigating. The patient data consists of next-generation Illumina reads (MiSeq or NextSeq) from captured exomes for a panel of a given size (small number of genes). For example, when looking at the gene panel used to diagnose the heart disease cardiomyopathy, there is sequence data available on “only” 55 genes out of the 26k genes. (CARDIO panel, see table and the NCBI product page - note that it only lists 51 of them) which are likely involved in the disease. The total length of the captured exons for these 55 genes is about 320,000 bases. Which, again, is just a very small fraction of the complete exome lenght. The first step will be a thorough quality check of the available data, for which we first need to take a close look at the data format being used. 1.7.3 FASTQ format As mentioned before, a great portion of the data produced in computational biology is from so called Next-Generation Sequencers. These machines read DNA or RNA material and write these sequences to a file (the different machines and techniques used to create such data will be presented during the Theory of Bioinformatics classes). A file format that you will often find is the FASTQ format. You can recognize such a file by it’s extention .fastq or .fq. A FASTQ file is a simple text file in which the read bases together with the predicted quality are stored. Below you will find two (shortened) reads coming from an Illumina next-generation sequencer. Keep in mind that a typical run on a NGS machine has millions of these short sequences! This theoretical maximum number of reads of a given size is determined by the underlying technique used, which greatly differs between the different techniques. Read 1: @M01785:20:000000000-A3F6F:1:1101:16810:1655 1:N:0:2 NTCATGTACGGTCAGGATGGACGCACTCAACATTTTCAAGTTATTACTCCTTCAACTCAAAACT … + #&gt;&gt;1A1B3B11BAEFFBECA0B000EEGFCGBFGGHH2DEGGHGFFFGFFHHFGBGEFFFFF … Read 2: @M01785:20:000000000-A3F6F:1:1101:12839:1664 1:N:0:2 TATATCTATGTCATTTTTTTCTCAATAATACTAAGAGAAAGAAGGCAACTCAAGGATCCTATTAATCCTTTA … + 1&gt;1AFFFD3DDDGGGGGGGGGHF3FDFGFHHFB1110FF10000FGGGHHDC110FEGGBGHFFHFHHHHGBFH … Each read constists of: A first line describing the machine it was run on, the chip identifier and the actual coordinates from the chip where the base was read The actual read sequence The plus sign The predicted read base quality ASCII value The difference with the widely known FASTA format is mainly the addition of the Quality line (green). So, given these quality-characters, how do we determine if the above sequences are good sequences? Each character in the quality line corresponds with a numerical quality score, which can be looked up in so called ASCII tables. Note: can you think of why this intermediate ASCII table is used to calculate quality scores? Lets take a closer look at the first sequence from above. The first 10 bases are: NTCATGTACG and the quality scores for these bases are: #&gt;&gt;1A1B3B1. We can now look up these quality characters in the ASCII table. If we look for example at the first character #, we find the value 35 in the ASCII table. For Illumina reads we have to subtract 33 (this is called the offset, and different techniques may use a different offset) from this value. So we end up with: 35 - 33 = 2. So the score for the first base N is 2. This score is called the Phred score. Lets also look at the Phred score for the second base T which has the ASCII character &gt;. The &gt; character translates to the value 62. Again subtract 33 from this value to calculate the Phred score. 62 - 33 = 29. What does the Phred quality score really mean? The score indicates the probability that the base call is erroneous. The quality score Q is logarithmically related to the probability of an incorrect base call: \\[ Q = log10P \\] or \\[ P = 10^{(-Q/10)} \\]. To calculate the probability that our first base was incorrectly called, we can calculate it like this: \\[ Q = 2 \\rightarrow P = 10^{(-2/10)} \\rightarrow P = 0,63 \\] which equates to 63%. We can also look for the probability that the first base was correct, then we have to subtract that number of 1. So the probability that the first base was correct is: \\[ 1 - 0,63 = 0,37 \\] or 37%. The probability the second base was correctly called is: \\[ 1 - (10^{-29/10}) = 0,9987 \\] or 99,87% accuracy. In general we can say that any Phred-score above 30 is acceptable (a 99,9% accuracy) which both the first two bases fail to get. Complete the table below. Base N T C A T G T A C G Quality char # &gt; &gt; 1 A 1 B 3 B 1 Numerical score ASCII value - 33 2 29 Base call accuracy 1 - P 37% 99,87% 1.7.4 Quality Control We need to load the patient data you are going to work on during this project. All patient data is stored on the network and you first need to load it into your Galaxy History to work with. To do this click on the ‘Upload Data’ button and next on the ‘Choose local files’ button at the bottom of the popup. Here, you can browse to the folder on the network that contain all samples which is: /commons/Themas/Minor01/samples Note that for Cardiomyopathy, use the CM-DNASeq folder and for Medulloblastoma, use the MB-DNASeq folder. As you can see each case consists of two files. This is because the forward and reverse reads are split up into two files (paired-end sequencing). The files with R1 or _1 in the name are the forward reads and the reverse reads have R2 or _2 in their name, remember this as some tools require to select the proper file. If for example Medulloblastoma is your disease case, you can select both read files from a Medulloblastoma sample, such as SRR29613194. Galaxy automatically tries to detect the file format that you will add to a history, most often using it’s file extension. In this case, we need to make sure that Galaxy has the proper format selected which is fastqsanger. The Sanger part of this format refers to the phred quality scoring method used in this file as there exist multiple scoring methods. Without the correct data type a tool such as FastQC (see below) doesn’t know when a read is of sufficient quality or worse, qualifies reads incorrectly. Select the proper type from the drop-down menu and upload the files. After you have uploaded the sequence data in your history, it is available to work on. If all went correct, you should now see the two sequence files in your History of Galaxy. When you start to analyse NGS data it is very important to get a feeling for the data. What does the data look like? What is the quality of my data? You do not (although you can) want to draw conclusions on low quality data. For some common errors the data can be corrected. To be able to do so, you would first need to identify what is wrong with the data. To look at many (quality) aspects of our imported data sets, we are going to start a tool named FastQC. In the Tools panel, search for the FastQC tool (can also be found under the FASTQ Quality Control section. The settings of the FastQC tool should appear in the middle section. A help is usually shown when you open a tool as well as the scientific literature associated with the tool for further references. For the FastQC tool we can load short read data from our history. Also a contaminant list can be uploaded (for example primers from a pre-process step can sometimes end up in the data and give all sort of problems downstream). In the submodule part of the FastQC setting you can specify which subparts of the tools need to be run. For now we are not going to bother you with settings other than the short read data. Select Multiple Datasets under the Short read data from your current history and press Execute. When you clicked Execute, the jobs were started and added to the History. Each item in the history has a number which increments with one for each new item. When the jobs are finished they appear in green. When the jobs are finished, four new items have appeared in the History. If you look closely, you can see that FastQC has been run on data 1 and 2 from the History. FastQC has generated two types of reports: RawData and a Webpage. We will have a look at the Webpages for both short read data sets. Click on the View Data (eye-icon) for the webpage output for data 1 In the middle panel the webpage with the results for data 1 should open. A summary of the quality checks performed is visable and gives you a quick overview of the checks that need your attention. Report on the total number of sequences analysed, the number of sequences flagged as poor quality, the sequence length and GC percentage. Next, look at the per base sequence quality, in the plot the sequence position is plotted on the x-axis and the base quality is plotted on the y-axis. Each bar in the plot represents the distribution over all sequences analysed in your data set at this position. Also look at the Webpage from data 2 and compare the plots. Do you see any differences between the plots and would you use this data set for further analysis? For further information on the box plot, please have a look at: https://en.wikipedia.org/wiki/Box_plot Now that we took a first look at the data and its quality, we will modify the data to make sure that we get data of the highest quality to perform our analysis on. Most likely the current data contains (many) reads of low quality or which are too short for performing the next steps. In the following two chapters we will finalize our look at the data quality by performing read trimming to remove these reads (or parts of reads) with low quality. Once we have a data set of acceptable quality we will perform the next and most important analysis step; read mapping (week 2), then we visualize the resulting mapping and perform a pileup operation (week 3. As you can see in the FastQC report, the read quality drops at the end of the reads (this is normal for the Illumina protocol and has to do with the specific sequencing technology used!). Can you find why this is happening? Also the spread in quality at some positions may be greater than you would like to see. In the next step we are going to remove reads that are of low quality or are just too short to be used. To make the DNA available to be sequenced, primers have been annealed to the DNA. These primers should also be removed. From the Tools menu select the Trimmomatic tool. Check if the paired end data is selected. Select Pair of Datasets as the Input Type (default) and select your patient R1 and R2 files. Perform an initial IlluminaCLIP step and select the TruSeq3 (paired-ended, for miSeq and HiSeq) adapters. From the Trimmomatic operation select the Sliding window trimming and choose 4 bases to average across, the quality should be 20 (as a minimum). Click on the plus sign (Insert Trimmomatic Operation button) and Select the Drop reads below a specified length Set the minimum read length to 70 (can you think of a reason why you do not want to have too many short reads?) Note: this is from the protocol used with the Cardiomyopathy data (paired-end 150bp reads). When using data with shorter reads (50, 75 or 100bp), change this setting accordingly (30, 40, 50 respectively). Execute the tool. Trimmomatic will give 4 new files as output, read the help of the Trimmomatic tool (can be found below the tool settings in Galaxy) to understand what each file contains. For next steps in our analysis it is very important that the forward and reverse reads are in the same order in the files. If for any reason one of the forward or reverse reads was removed from one of the files, the two files will not be in order any more. For this reason Trimmomatic will remove both reads from the files. If one read was below the threshold and was removed, it’s paired partner will be written to an unpaired output file (if it was above the threshold). In the output you will find 4 files: one for the forward paired and one for the reverse paired (these are still in order) and one for the unpaired forward and one for the unpaired reverse. Rename the two files that you will use for the next step to something like Trimmed Reads [ID] R1 00X etc. Note on Reporting Galaxy Tool Parameters An important aspect of our lab journal is to document reproducible research. In order to do that, we need to properly document the tool parameters used for tools operating on our data. This is especially important for tools that modify the data, like Trimmomatic. Tools often have many parameters and we are not going to manually write down all of them. Instead, we will copy the ‘Tool Parameter’ table from the Galaxy tool log and paste that into our lab journal. Here is a suggestion on how to accomplish this: Click on the ‘i’ icon in the history item for the Trimmomatic tool (named ‘Dataset Details’). Select the complete ‘Tool Parameters’ table (including the header, see this image for example). In RStudio, click on the ‘Visual’ button in the top left of your markdown document Paste the copied table into the markdown document. Modify the table if needed and - preferably - return to the ‘Source’ editor. The final result should be similar to the table below (some rows removed): Input Parameter Value Single-end or paired-end reads? pair_of_files Input FASTQ file (R1/first of pair) 3: SRR26913192_1.fastq.gz Input FASTQ file (R2/second of pair) 1: SRR26913192_2.fastq.gz … … Select Trimmomatic operation to perform SLIDINGWINDOW Number of bases to average across 4 Average quality required 20 Select Trimmomatic operation to perform MINLEN Minimum length of reads to be kept 30 Report on the number of reads that where removed for each data set. Create new FastQC plots of the Per Base Sequence Quality (on the cleaned, paired data). Compare the plots with the original uncleaned data and report on the differences. Perform multiple FASTQ cleaning runs (each time on the original data!). Change a setting and report on the number of reads removed. Create and save the Per Base Sequence Quality plots for each run. Report (in a table for instance) what the effect is of each setting in the Trimmomatic tool. Pick the best (combination of) settings and copy the R1 and R2 trimmed data sets to a new history in Galaxy, this data will be used for the next analysis step. Please remove any unused output of Trimmomatic from the previous history, as this might take up a few gigabytes of space. If you delete these files (they can also be ‘undeleted’) they are just marked for actual deletion after a few months. "],["mapping.html", "Week 2 Read Mapping Mapping with BWA 2.1 Assignment week 2 2.2 Marking Duplicate Mapped Reads 2.3 Visualizing the Mapping Data", " Week 2 Read Mapping Now that we know what the quality is of the NGS patient data and have corrected the reads on low quality and short read lengths, we can continue with the mapping of our reads against a reference genome. In this step we use the human genome as a reference to which we will map the cleaned reads using the tool BWA. After mapping the reads we can look for variations between the reference and our patient. Mapping with BWA Before starting with mapping the data to the human reference genome, we can calculate some important statistics to see what we can achieve with our data. 2.1 Assignment week 2 The coverage is the number of reads that were mapped at a given position and it is considered a good measurement to determine if there was enough data for further analysis. To identify significant variations, we aim for a minimal coverage of 20, any lower and it becomes hard to differentiate between a true variant and a sequencing error. Given the Illumina platform and given these facts: a minimal average coverage (read depth) of 20, the read length is determined by your data set (for instance a read pair with each read having 75 or 150 bases), the human genome size is 3,137,161,264 bases and the target region (captured region, size of the gene panel) totals 320,000 bases for the Cardio panel or 1,000,000 bases for Medulloblastoma. This differs for your chosen case and has to be calculated by you. We can use the Lander/Waterman equation to calculate our actual expected coverage: \\[ C = LN / G \\] Where: C is the coverage, L is the read length and N is the number of reads From your own cleaned data set, look up how many reads are left for mapping. Calculate the expected coverage if we use this data for your gene panel (G = captured region). An Illumina MiSeq V3 can produce up to 25 million reads of length 300, how many patients could you analyse per run if the minimum coverage is 20 and you where using your gene panel? Calculate these assignments in R within your document. Select the correct Map with BWA tool from the Tools menu (depends on your read length). For the mapping we will be using a built-in genome of which there are many available. Select the Human reference hg38 in the reference genome setting. Select paired end reads. From your previous experiments with different FASTQ cleaning settings, select the R1 paired Trimmomatic data as the first set of reads and the R2 paired Trimmomatic as the second set of reads. Execute the tool, this will take a while to run (10 - 20 minutes). 2.2 Marking Duplicate Mapped Reads In the process of creating the reads, duplicates may have arisen by PCR artifacts. These duplicate reads are not real biological sequences in the sense that they originate from the sample, but are a technical artifact. In downstream analysis these duplicate reads may generate false positive variants. Can you think of a reason why this is the case? Before we are going to look at any differences between the reference and our patient, we first have to mark the duplicate mapped reads. To do this, select the MarkDuplicates tool from the Tools menu. Select the Map with BWA-MEM output on data … and …., set the Assume input file is already sorted option to No and Execute the tool. This tool will add a flag to each read that it finds as being duplicate and other tools will ignore any read that has this flag. It will therefore not remove the read from the data. 2.3 Visualizing the Mapping Data We are going to look at the actual mapping to get a bit of feel for what has happend till now. To do this we will look at the mapping output from the previous step - with the marked duplicates - in a Genome Browser. On our system the Integrated Genome Viewer (IGV) has been installed. First we need to download the mapping data to our computer. To do this, download the dataset and bam_index files from the markduplicates output in Galaxy as shown below. Select Save File in the pop up window. Open IGV either by going to the (Linux) Applications Menu -&gt; Run Program… and type in igv and click on launch or by opening a terminal and entering the igv command. Next, you can load the mapping data into IGV by clicking on File -&gt; load from File…. Look in your Downloads folder for a file name starting with Galaxy and ending with .bam (you only need to open the BAM file, the index file is automatically loaded). Because our sequence reads are from captured exomes, you have to zoom in quite a bit to see any of the mappings. Too help you find where to zoom in, we can add an extra layer to the genome browser (called a track). The Galaxy server hosts the details of the available gene panels for that purpose. You can download the file for your case by going to Shared Data -&gt; Data Libraries -&gt; case_name in the Galaxy browser. Select case_name.BED and click on the to History button (replace the case_name with your actual case). Please have a look at the file in you History. The file consists of 4 columns, which describe the chromosome number, exon start location, exon end location and the gene name. Download this file (Save File) to your computer. Now, in IGV, again select File -&gt; Load from File... Select the downloaded BED file and open this. IGV will now display a new track containing all exons of interest (for the SOD2 gene in this example): From the 4th column in the bed file, choose a couple of gene names to inspect. The shown SOD2 gene is located on chr6 and has 5 exons and is part of the Cardiomyopathy gene panel. In IGV type in the name of your selected gene in the search box and click on Go. The screen will load the mapping results of the region that includes the selected gene. A couple of regions are important in this genome browser screen. The top row shows the location you are looking at now (red vertical bar on the chromosome): The bottom tracks show the locations of the reference human genes and the locations of our gene panel captured exons. Note that due to splice variants, not all exons are of interest; in this example, the last exon (first one from the left, the SOD2 gene is on the reverse strand) is not part of the gene panel: The middle part is the actual mapping data of which the first tarck shows the coverage plot showing how many reads where mapped at this position and what the nucleotide distribution is at this position (when hovering on a position with the mouse cursor). Also the number of forward and reverse reads is shown. In this case at this position there where 328 reads mapped. It also tells us that 100% of the reads have a G at that position (157 in the forward mapped reads and 171 in the reverse mapped reads) Below the coverage track the actual mapping data is displayed. We mapped paired-end reads and to make this visible in IGV, right click the mapping track and select View as pairs. Reads are colored according to their read orientation and insert size. Look in the IGV online manual for the explaination of the colors. Zoom in on your gene of interest. Regions of reads that are grey of color indicate a similar region as the reference. Variants are shown by colored vertical bars (each nucleotide has its own color). Zoom in till you have the nucleotide sequence showing for a variant. In our example we are looking at a T variant for this patient at this position. We see that a total of 117 reads were mapped at this position and that from all reads 64 had a T and 53 had a A at this position. The patient is heterozygous for this allele. Can you see if this variant is in an exon or not?, what are the consequences of a variant in an exon location? Assignment: look for a variant in an exon within your own mapping data. The bottom row will show the translation from DNA to protein. Does the found variant causes a change (non-synonymous) or is the aminoacid sequence the same (synonymous)? Note that you need to take the gene orientation into account. The black arrow on the bottom left in IGV near the amino acid translation track is clickable to change the orientation. During the next steps we will answer questions for your own data set, such as: how many variants are found?, how many of these are located within exons? how many variants actually cause an aminoacid change? We will not do that manually using IGV as this tool uses a very naive variant calling method and this will result in many false positives. So we are going to use a more sophisticated method in the next chapter. "],["pileup.html", "Week 3 Analysis of Variants 3.1 Assignment week 3", " Week 3 Analysis of Variants Once we are done with mapping all reads to the reference genome, we can determine the variant positions within the genes of interest. For this we use a Galaxy tool called LoFreq that will report on found variants and their statistics such as allele frequency, number of supported reads, average base quality at the found position, etc. Before finding the actual variants, we will perform analysis assessing the achieved coverage. As you might have seen in IGV, coverages range from very high numbers (300+) to places where there are only a few reads present. In this chapter we will use R with a number of libraries often used for genetics research to report on these numbers for all panel genes. 3.1 Assignment week 3 Programming Assignments; Coverage Overview for Panel Genes Within the output of the previous mapping step (the BAM file) we can see how (well) our panel-genes are covered by our reads, a very important statistic on the quality of our data. We could assess this manually using IGV, but that is definitely not the method to use for such a task given that there can be 1-2000+ mapped exons in total. This section contains a number of small assignments that - once completed - will give us a clear indication whether our mapping is of sufficient quality. Depending on this outcome we might need to revisit our previous step(s) such as the trimming process to increase our mapping quality. During these assignments we will gain insight into the coverage of all of our genes of interest as well as their variants. We will use the BED-file describing the locations of all exons, the BAM file containing the mapped reads and - which we’ll create later - a VCF-file listing all found variants. A number of assignments asks to visualize the results to get a good understanding of what we’re looking at. Assignment Instructions Below are a number assignments that together perform the tasks as explained above. For each you are required to carefully read the instructions and make sure you understand the goal and data used for that particular assignment. Being able to critically reflect on the outcome of your analysis is the most important aspect of being a bioinformatician! Furthermore, these assignments are generally a guideline and while it should be considered the minimum tasks to complete, you are always invited to do more with the data that you have. Feel free to explore and keep that code in your lab-journal as well. To perform some of these tasks we do need to learn some theory that is specific for working with our data (BED and Pileup files among others). For that we will introduce the Bioconductor libraries later on. First though we will explore the most simple of our data files and create a few simple plots based on the BED data. For each of these assignments, create at least one code-chunk, name it accordingly and in between the assignments reflect on what you did. For instance as the end product of assignment one is a data frame with a proper header and an overview plot of the number of exons per chromosome, you could end this assignment by stating “After loading and processing the data, the overview shows all chromosomes on which exons are located for the panel genes”. 3.1.1 Assignment 1; Loading the BED-data The first step of our program is to read the BED file to get the exon locations for our panel genes. We already viewed the contents of the BED file in Galaxy, where we saw that this file contains 4 columns: The chromosome number, exon start location, exon end location and the gene name For example: 10 75399683 75399795 MYOZ1 10 75757946 75758153 VCL 10 75802821 75802931 VCL Perform these tasks: Load the BED-data Since this data is stored in a text file using tabs as column separators, we can easily read this in using the read.table function. Change the column names There are no column names in the BED-file and read.table gives default names in this case: [1] \"V1\" \"V2\" \"V3\" \"V4\". Using the names function, we can assign new column names, do this by assigning a vector with four column names (see the help of names) Print the number of exons per chromosome An easy way to get an overview of large amounts of data (only 1204 exons in this case) is by using the table function. Using this function, show the number of exons per chromosome. Plot the number of exons per chromosome The data from (3) can be used to create a plot by giving it to the barplot function. Change the label on the y-axis to something meaningful. Since the chromosome numbers can’t all fit on the x-axis you can change its orientation using the las = 2 argument to barplot. Further options aren’t necessary and the plot doesn’t have to be very pretty at this stage. 3.1.2 Assignment 2; BED-visualization Visualize the number of genes per chromosome This requires the creation of a subset of unique chromosome-gene combinations; all genes consist of exons but if we reduce all genes to a single exon (doesn’t matter which), we can then use the table function again to give us the numbers that we need. Select only the ‘chromosome’ and ‘gene’ columns (using the names that you gave in assignment 1.2) from the BED-data Apply the unique function to reduce this data to a single row per gene and store only the ‘chromosome’ column of the result Give this data to table and use that result as input for the barplot function using a proper label for the y-axis. 3.1.3 Assignment 3; Bioconductor As software developers develop algorithms or solutions for common problems, they would like to publish their code so that others can use it too. For R (and many other programming languages) this code is packaged in a library that can then be easily distributed to others. You might already be familiar with the library() function in R to load external code into your environment and that is what we will do here too. Bioconductor is a repository that contains over 1800 of these libraries that we can get into R and use to perform analysis with. This vast amount of libraries are all focused on analyzing, processing and visualizing (high-throughput) biological data which we will also be using during both practical courses. Luckily you don’t have to find the libraries yourself and the assignments will always instruct on which one to use (to load actually, since they are pre-installed on our network). What we will introduce here is a concept that is required for most of the remaining assignments below, namely GenomicRanges objects (website, manual), consisting of IRanges (website, manual) objects. These ‘objects’ (they hold data in a combination of vectors, lists and data frames among other data structures) are used to store information about features on the genome. These features (introns, exons, variants, etc.) are always defined by their coordinates such as start and stop positions and the name of the sequence (in our case, chromosome) on which they lie. The BED-file for instance can be represented using IRanges since it contain simple positional data (chromosome and position (start/ stop for BED-data). Let’s first see an example of representing a few exons from the BED file into IRanges objects (you don’t need to replicate this example). ## Only needed when the package is missing if (!require(&quot;BiocManager&quot;)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;GenomicRanges&quot;) # This package contains the &#39;IRanges&#39;, &#39;GRanges&#39; and &#39;GRangesList&#39; classes we will use library(GenomicRanges) # Loading the BED data (note: use your own data from assignment 1) load(&quot;data/bed_data.RData&quot;) # Print all rows where the gene name is for example &#39;SOD2&#39; bed_file[bed_file$gene == &quot;SOD2&quot;, ] ## chromosome begin end gene ## 649 6 160103505 160103690 SOD2 ## 650 6 160105866 160106085 SOD2 ## 651 6 160109138 160109294 SOD2 ## 652 6 160113673 160113915 SOD2 ## 653 6 160114157 160114219 SOD2 Shown above is a dataframe similar to your data containing the five exons of the SOD2 gene located on chromosome 6. For each exon we have a begin and end coordinate. This data can be converted into IRanges by using the similar named function and providing the correct columns as shown below: # Get the data as a subset of the cardiopanel data sod2 &lt;- bed_file[bed_file$gene == &quot;SOD2&quot;, ] # Assign whole columns to the IRanges arguments ranges &lt;- IRanges(start = sod2$begin, end = sod2$end, names = sod2$gene) # Print the object ranges ## IRanges object with 5 ranges and 0 metadata columns: ## start end width ## &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; ## SOD2 160103505 160103690 186 ## SOD2 160105866 160106085 220 ## SOD2 160109138 160109294 157 ## SOD2 160113673 160113915 243 ## SOD2 160114157 160114219 63 # Calculate and print the length of a gene cat(&quot;Length of SOD2:&quot;, sum( width(ranges) ), &quot;bp&quot;) ## Length of SOD2: 869 bp We see the same data now represented as an IRanges object. We do however both miss some data (the chromosome) and get some extra data as well (the width (length) of each exon). As it’s important to know on which chromosome each gene lies we will convert our ranges object into a GRanges object that combines both data: ## GRanges object with 5 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## SOD2 6 160103505-160103690 * ## SOD2 6 160105866-160106085 * ## SOD2 6 160109138-160109294 * ## SOD2 6 160113673-160113915 * ## SOD2 6 160114157-160114219 * ## ------- ## seqinfo: 18 sequences from an unspecified genome; no seqlengths # Converts iRanges to GRanges, adding the chromosome name granges &lt;- GRanges(seqnames = sod2$chromosome, ranges = ranges) granges When comparing the IRanges object with the GRanges object, it looks rather similar with yet again some missing data (the width) and some extra data (the strand). Nothing is lost though, as objects often have a lot of accessor methods that you can use for getting to the actual data, as the example above is a representation of the object that you get when you print it. Functions such as width() applied to an IRanges object gives the width column and the ranges() function applied to a GRanges object gives the IRanges object contained within. When needed, the assignments will hint to their usage but the linked manuals also describe the most important ones. Now that we have a gene within a GRanges object, what can we do with it? For now with just the BED data not a lot, but as we will also put the Pileup data into a similar GRanges object, we can start asking our data interesting questions with the pileup function for instance. This can be used to get the Pileup information specific for our exons and ignoring everything else. But before we can do that we need to get our BED-data into the proper format first and that is what we will do now. This more challenging assignment results in yet another type of object, named a GRangesList that contains multiple GRanges objects; one for each gene such as shown in the above example. So, in order to do that we need to: Create an empty (standard) R list, Iterate over all of our genes in the BED file, Read the part about iteration below and the hints about the split function to get the gene names and data easily accessible. For each gene, create a GRanges object and add it to the list, including the gene name (use the following form: my_list[[gene_name]] &lt;- data) and Convert the complete list into a GRangesList object. Use the conversion as explained in the GRanges manual: bed_data &lt;- GRangesList(bed_data) given that the list is called bed_data. The techniques required for this assignment are mainly knowing how to work with lists (including how to create them, accessing single elements, adding to lists and iterating over its items). The following hints, the numbered list above and the examples are everything you need to do this assignment. Hints and further instructions: Iteration When we talk about iterating when programming, we mean repeatedly performing a set of operations on separate elements (of a vector, a list, columns or rows of a data-frame, etc.). In ordinary programming languages this often requires explicit step-wise execution of these operations, using constructs such as for and while ‘loops’. In R however, we already perform a lot of iterative steps without realizing it. Given a vector x, we can multiply each value with itself by simply stating x * x. Programming languages such as Python, Java and C require code like: squared = [] for value in x: squared.append(value * value) This is still pretty readable code, but in R we never require this explicit form even though we could. This simple one liner in R for instance calculates the average log2 ratio of two columns from a data-frame that can contain thousands of rows and many columns, at once: mean(log2(dat[&#39;sample1&#39;], dat[&#39;sample2&#39;])) which would require a rather large for-loop in most other languages. When we do want to apply something to each element separately in R we often use the - rather complicated but efficient - apply functions. For now though we will use the easier to understand for loop to perform our code on data for each gene (point 1 above). With the following example on iterating over our bed_data you should be able to perform this assignment: # Load the stored data; do not repeat. Object name in this # example is &#39;bed_data&#39; load(&#39;data/bed_data_GRangesList.RData&#39;) # Split the data by gene, resulting in a list with named elements bed_splitted &lt;- split(bed_file, bed_file$gene) # Print the names of the first 10 genes for (gene in names(bed_splitted)[1:10]) { print(gene) } ## [1] &quot;ABCC9&quot; ## [1] &quot;ACTC1&quot; ## [1] &quot;ACTN2&quot; ## [1] &quot;ANKRD1&quot; ## [1] &quot;BAG3&quot; ## [1] &quot;CALR3&quot; ## [1] &quot;CAV3&quot; ## [1] &quot;CRYAB&quot; ## [1] &quot;CSRP3&quot; ## [1] &quot;DES&quot; Hints As point 2 asks to iterate over all genes, we need to think of a way to group our data (exons) into genes which can be done using the split function. The result of split is a list with 55 named elements; one for each gene. The split function requires two arguments: first the data-frame containing all of the data and the second argument is the column containing the gene names (i.e. bed_file$gene) You can get the gene names by executing the names function on the resulting list from split. You can get all the exon-rows easily by accessing the list items: split_output[[gene_name]]. When iterating, the code that you execute for each gene could be (1) create an IRanges object, (2) create a GRanges object combining the chromosome with the IRanges object and finally appending this to a list. Note that this requires an empty list to be created before the for-loop, such as exons &lt;- list(). Revisit everything you’ve written to complete this task and make sure you understand what everything does (the exact how it works is of less importance). Also, you can use functions like class and elementType to confirm that your list is of class GRangesList (actually a CompressedGRangesList) and its contents are of type GRanges. 3.1.4 Assignment 4; Gene Lengths We’ll create one last visualization based on the BED-data - but now using the GRanges object - namely an overview of the gene lengths; another barplot. The following example code shows how to calculate the length of a single gene (using ranges, width and sum). The same code also works on the complete GRangesList object, use this to create a barplot showing the length of each gene. # Check the length (in basepairs) of a single gene (exons only) psen2_ranges &lt;- ranges(bed_data[[&#39;PSEN2&#39;]]) paste(&quot;Length of the &#39;PSEN2&#39; gene:&quot;, sum(width(psen2_ranges)), &quot;\\n&quot;) Hint: As you might see, there can be some genes which are very long compared to the others and this makes the plot difficult to read. The plotrix library contains the gap.barplot function specifically for this purpose (also supports the las = 2 argument). Try to use this function for creating the barplot. As a small challenge you can try to style the y-axis to show more values (‘ticks’) besides the minimum and maximum. 3.1.5 Assignment 5; Loading the Mapped Reads In R we will use the output of the mapping step (the BAM file) to inspect the coverage of all mapped positions of our panel genes. There is an R library that can read this data (RSamtools website and manual) into GRanges objects. We have already downloaded the data as it was the input for the IGV tool in the previous chapter. Rsamtools Again, we have to briefly introduce this library before you will be able to use it. Rsamtools is described as “an interface to BAM files”, the file that contains all the mapping information. # Installing required library BiocManager::install(&quot;Rsamtools&quot;) # Loading the RSamtools library library(Rsamtools) Functions from Rsamtools that we will use - in order - are: BamFile: Reference to the BAM data and its index (.bam.bai) ScanBamParam: Sets the parameters for selecting a certain region from the genome using GRanges (i.e., a gene from our BED-data) pileup: Actually retrieves the region of interest given the BamFile and parameters supplied. Assignment: Load the BAM mapping data into a list of data-frames covering all mapped positions of interest Instructions Create a BamFile object by providing both the file and index parameters Create a ScanBamParam object What this does is defining all the ranges that we want to get from the mapping, in this case the ranges of all exons for a gene Do this by selecting a single gene from your GRangesList object (using the [[\"geneName\"]] syntax) and store this in an object (it’s type will be a GRanges) Give this GRanges object to the ScanBamParam function and store this in a variable Get the actual mapping data for our region of interest Call the pileup function providing - in order - the objects created at points 1-2. Note that you need to tell pileup what the parameter is, so use pileup(file = ..., scanBamParam = ...) The output of the pileup function is a data-frame, store this result Print the first lines of the data-frame (use the head function) The mention of pileup in this assignment means looking at a single genomic position like we’ve seen in IGV. There are tools available to generate a pileup file from a BAM file (file format description available here and here) containing the following information for all mapped positions: the chromosome name (1) reference position (2), reference base (3), number of supporting reads (4), the match (5), either: a dot for a match in the forward strand or a comma for a match in the reverse strand a nucleotide in case of a mismatch (upper case for the forward strand, lower case for the reverse strand) insertions/ deletions (defined by a + or - sign followed by a number defining the size of the insert/ deletion, followed by the bases that form the insert/ deletion). the quality [6] (taken from the FASTQ file) can be seen in the last column, one value for each read 1 2 3 4 5 6 chr1 10004 c 1 , [ chr1 10005 g 3 ,,. ?1&gt; The reason this format is called pileup is because for each position it will pile (or stack) the data from all mapped reads for that position. In the first example line above (chromosome 1, position 10004) there was only one read with a match (the reference is a C as shown in column 3) in the reverse strand and the second line has three supporting reads with a match. The line below is an example where multiple reads (30 in total, as shown in column 4 and by the lengths of columns 5 and 6) mapped on a single position. Here on chromosome 1 we see that in column 5 there is something going on. Read 8 contained a C whereas the reference (and the other 29 reads) show a T at this position, so this is classified as a mismatch which will be the subject of further analysis steps. chr1 16888646 T 30 .......C........,.,.....,,.... WomHHGomooHmHFHHGGFH1GDHHHHGFF Besides mismatches, we can also have insertions and deletions such as shown in the example below with a - sign followed by a number and a number of bases that are missing. chr1 16888710 a 31 ..,.,..,..,,-2ct,,..,.,,.-2CT,,.,,.,.,, nGFkGUlFF?DFHFGGEnHFFHHGFHG5GGB Look at the data that we get for each position and note that we have - at least - two rows of data per position; one on each strand (as we’ve also seen in IGV). If we have more than two rows of data for a single position, it might be a variant which is coincidentally demonstrated in the example data below: # Load the example Pileup data for the SOD2 gene (do not repeat) load(&quot;data/sod2_pileup.RData&quot;) head(sod2, n = 5) ## seqnames pos strand nucleotide count which_label ## 1 chr6 160103505 + C 221 chr6:160103505-160103690 ## 2 chr6 160103505 - C 77 chr6:160103505-160103690 ## 3 chr6 160103505 + T 1 chr6:160103505-160103690 ## 4 chr6 160103506 + T 220 chr6:160103505-160103690 ## 5 chr6 160103506 - T 77 chr6:160103505-160103690 The first position of the SOD2 gene in this case shows three rows; two on the positive strand and one on the negative strand. The count column shows that there is a single read that has a T nucleotide at this position instead of a C that the other 297 reads show. With such a big difference, this can be attributed to a sequencing error. Note though that what we have now is a summary of the actual pileup compared to the very detailed line we see in the actual Pileup file within Galaxy. This data is also available using RSamtools but we only require this summary. 3.1.6 Assignment 6; Pileup Processing In this assignment we will get the pileup data for all of our genes into a single object so that we can report on the average coverage of each gene and determine if there are positions with a low coverage that we might need to inspect, more on that later. Steps that need to be taken: Create an empty list (this will hold a separate data-frame for each gene as created in assignment 5) Iterate over your BED-data (the GRangesList object) similar as we did in assignment 3 Get the pileup data for each gene in the for-loop, by creating a ScanBamParam and executing the pileup function Add the resulting pileup data-frame to the list, using it’s gene name as identifier (pileup_list[[gene_name]] &lt;- ...) Check by printing the head again for the same gene as used in assignment 5 3.1.7 Assignment 7; Coverage calculation and reporting Calculating the actual coverage for each position is a little cumbersome as we need to sum the count of all rows for a single position since we don’t - yet - care about mismatches or the strand a read is mapped on. Instead of performing all kinds of programming steps to check if positions are similar and if so, summing the count values, we can use an R function to do this for us. We don’t expect you to understand the following example besides understanding what it produces. This summation can done by a function called aggregate, defined as “Compute Summary Statistics of Data Subsets”. Given a column to group on (our pos column) it will perform a function on a different column (our count column containing the coverage. The result of aggregate is a two-column data-frame with the position and the sum of all count values for this position; the total coverage. We provide the arguments as follows: x: the count column (we provide it as a list so it gets a name in the resulting data-frame as well) by: the pos column the grouping is applied to FUN: the function to perform on all grouped rows (in this case, the sum function) Use the code below but check and rename the used variables to suit your data. coverage = aggregate(x = list(count=sod2$count), by = list(pos=sod2$pos), FUN=sum) head(coverage) ## pos count ## 1 160103505 299 ## 2 160103506 297 ## 3 160103507 297 ## 4 160103508 291 ## 5 160103509 292 ## 6 160103510 290 Your task now is to calculate this per-position coverage for all genes and to calculate and report on - for each gene: the number of bases sequenced for this gene, the average coverage of the gene and the number of low-coverage positions (coverage &lt; 30) both in number and as the calculated percentage. Given these tasks and your code from previous assignments, you should be able to do this. Store the calculated values (4 values) combined with the gene name in a new data-frame and show it completely in your lab-journal, ordered by the percentage of low-coverage positions, descending. The following code can be used to create an empty data-frame and add rows to the data. Note that this isn’t done very often, use and forget. The first line of code (the options function) is needed to stop some annoying R behaviour. # Tell R not to complain... options(stringsAsFactors = FALSE) # Create empty data-frame, but specify columns and data types df &lt;- data.frame(Name=character(), Course=character(), Grade=numeric()) # Create a new row of data, stored as a named list (important) new_row = list(Name=&quot;James&quot;, Course=&quot;Math&quot;, Grade=7.5) # Append data to the data-frame using rbind df &lt;- rbind(df, new_row) # Or within a single line df &lt;- rbind(df, list(Name=&quot;Wendy&quot;, Course=&quot;Math&quot;, Grade=8)) # print df ## Name Course Grade ## 1 James Math 7.5 ## 2 Wendy Math 8.0 Hints As with the other assignments using iteration, create the data-frame outside of the for-loop and perform the rbind within the for-loop. Take a good look (and print while testing) at what you are iterating over; in this case a list containing data-frames. You need to re-use the aggregate code but adjust it to make sure it uses the data you get each iteration Store each calculated value in a variable within the for-loop and check (print) them as mistakes are easily made. End Result Below is an example of data you might get when you have completed this assignment. Note that all values as well as the column names will be different in your case. # Load example data load(&quot;data/statistics.RData&quot;) statistics ## length avg_coverage low_coverage low_coverage_perc ## ABCC9 6348 249.2 0 0.0 ## ACTC1 1375 192.4 57 4.1 ## ACTN2 3527 184.5 0 0.0 ## ANKRD1 1322 232.4 0 0.0 ## BAG3 1892 207.7 0 0.0 ## CALR3 1511 242.5 0 0.0 ## CAV3 536 196.0 0 0.0 ## CRYAB 650 156.8 0 0.0 ## CSRP3 785 230.6 0 0.0 ## DES 1774 122.9 258 14.5 ## DMD 14643 248.7 0 0.0 ## DSC2 3424 237.9 109 3.2 ## DSG2 3959 239.1 86 2.2 ## DSP 9577 237.4 4 0.0 ## DTNA 3133 238.0 0 0.0 ## EMD 1006 169.9 0 0.0 ## EYA4 2683 251.9 0 0.0 ## GATAD1 1010 183.3 210 20.8 ## GLA 1570 237.6 0 0.0 ## JPH2 2292 88.7 699 30.5 ## JUP 2758 78.2 276 10.0 ## LAMA4 7193 236.0 0 0.0 ## LAMP2 1957 240.1 0 0.0 ## LDB3 3148 144.0 78 2.5 ## LMNA 2496 81.6 118 4.7 ## MYBPC3 5195 121.8 503 9.7 ## MYH6 7301 200.1 32 0.4 ## MYH7 7329 205.0 0 0.0 ## MYL2 781 211.4 1 0.1 ## MYL3 828 164.4 0 0.0 ## MYOZ1 1100 230.5 0 0.0 ## MYOZ2 995 252.1 0 0.0 ## MYPN 4723 243.7 0 0.0 ## NEXN 2511 247.7 0 0.0 ## PKP2 3206 207.9 263 8.2 ## PLN 199 233.8 0 0.0 ## PRKAG2 2350 183.2 349 14.9 ## PSEN1 1804 241.4 0 0.0 ## PSEN2 1747 150.2 56 3.2 ## RBM20 4243 215.7 230 5.4 ## RYR2 19104 246.3 88 0.5 ## SCN5A 6633 162.7 275 4.1 ## SGCD 1193 232.9 0 0.0 ## SOD2 869 184.4 6 0.7 ## TAZ 1400 168.5 0 0.0 ## TBX20 1664 193.4 167 10.0 ## TCAP 584 80.6 0 0.0 ## TMEM43 1683 179.5 52 3.1 ## TNNC1 726 175.0 0 0.0 ## TNNI3 953 105.2 0 0.0 ## TNNT2 1458 200.3 24 1.6 ## TPM1 1941 183.4 326 16.8 ## TTN 123059 250.0 0 0.0 ## TXNRD2 2241 132.8 116 5.2 ## VCL 4285 210.5 89 2.1 Make sure the complete report is visible in the lab journal. Reflect on the results obtained; are there any genes that require further inspection? If so, use IGV to check if the low-coverage positions are due to a lack of reads or if there are other reasons for this (include screenshot(s) in your lab journal). "],["variants.html", "Week 4 Finding and Annotating Variants Finding Variants 4.1 Variant Annotation 4.2 SnpEff and SnpSift 4.3 Programming Assignment; Analyzing Variant Annotation", " Week 4 Finding and Annotating Variants Most likely you’ve already seen how a true variant (not a single or few mismatches) looks when we looked at the data in the IGV-browser. For such a variant you see a number of reads having a different base at that positions compared to the reference genome. As we’ve mentioned before, we are interested in all these variants in our patient data for the all the genes in our gene panel. The previous visualize-your-data assignment did not ask to find all these variants using the IGV-browser because in this section we are going to use a program to do this for us. Finding Variants The tool that we will use is the LoFreq tool that scans the BAM file for variants. This tool has a few settings that, combined, defines when a position is called a variant. You could do this in a naive way and just report for each position if there is a change. But this will result in many false positive variants (can you explain this?). We need a more statistical approach to filter out low quality variants and that’s why the tool has settings to for instance set the minimum amount of reads (coverage) at a position to consider looking for variations. Also the variants itself should be supported by a minimum number of reads and the base quality of that position should not be too low. Instructions: Run the Call variants with LoFreq tool using its default settings. Select the output BAM file from the Mark Duplicate tool and use the available GRCh38 reference genome. Feel free to explore the other options but leave the Call variants across settings on Whole reference (this setting allows using a BED file for only selecting variants of interest which is basically the following assignment). VCF File Filtering The result of the LoFreq tool is a single new file in your history in the vcf file format where each line describes a single variant. In Galaxy you can then directly see how many variants you have; often well over 1000 in total. Note the 17 comment-lines at the top of the file. When looking at this file in more detail it is fairly easy to see variants in places we are not interested in. As you’ve seen in IGV, many regions have been sequenced outside of our genes of interest, or even very far from a gene at all. LoFreq also checked those regions for variants and this assignment asks to filter the list of variants only keeping those within exon boundaries of our genes of interest. Then, another filtering step is filtering on actual variants. We do this by looking at the allele frequency value included in the VCF file. In our case, this value describes the percentage of reads having the variant base. 4.0.1 Assignment 8; VCF File Processing Please briefly read the linked Wikipedia page to understand why this value is of importance. As we are working with patient data, we use the protocol as described by the UMCG that states that variants with a minimum frequency of 30% are retained. This means we will filter out any variants with a lower value. Note that LoFreq stores this value as a fraction so we filter for values \\(&gt;= 0.3\\). There are once again step-by-step instructions for completing this assignment. It is however possible to do it without these instructions as we partly repeat steps we’ve taken in the previous assignments. Read in the data into a data-frame using the read.delim function Make sure the data has a proper header (line 18 in the data) Provide the stringsAsFactors = FALSE argument, otherwise the next part won’t work (we cannot split an R factor) Now that we have the data in a data-frame, inspect that everything is loaded correctly. The first thing that we’ll do now is get the frequency value for each variant. Read the comment lines in the VCF file to see how this value is stored. You’ll see that the columns themselves contain more fields, separated by a semicolon (;) which we can use to split the data to get to the value we want. Split the column containing the frequency value using the strsplit function The output of this function is a list in which each item is a vector with the separate items resulting from the split (inspect this object in RStudio). To get the frequency value from this list, here is some R magic that converts the result from strsplit into a matrix (found on Stackoverflow): head( do.call(rbind, bed_splitted), n=20) ## chromosome begin end gene ## ABCC9.814 12 21953958 21954135 ABCC9 ## ABCC9.815 12 21958088 21958265 ABCC9 ## ABCC9.816 12 21958912 21959014 ABCC9 ## ABCC9.817 12 21960260 21960433 ABCC9 ## ABCC9.818 12 21962766 21962909 ABCC9 ## ABCC9.819 12 21964963 21965111 ABCC9 ## ABCC9.820 12 21967558 21967676 ABCC9 ## ABCC9.821 12 21968677 21968847 ABCC9 ## ABCC9.822 12 21970101 21970261 ABCC9 ## ABCC9.823 12 21971064 21971205 ABCC9 ## ABCC9.824 12 21981872 21982014 ABCC9 ## ABCC9.825 12 21990992 21991124 ABCC9 ## ABCC9.826 12 21995228 21995425 ABCC9 ## ABCC9.827 12 21997397 21997506 ABCC9 ## ABCC9.828 12 21997681 21997869 ABCC9 ## ABCC9.829 12 21998517 21998786 ABCC9 ## ABCC9.830 12 22001064 22001200 ABCC9 ## ABCC9.831 12 22005011 22005176 ABCC9 ## ABCC9.832 12 22005282 22005459 ABCC9 ## ABCC9.833 12 22012500 22012620 ABCC9 What it does is rbinds all vectors into a matrix. Again, inspect this object to see where the actual value is located that you want (use the View function in RStudio or click on the name in the Environment tab). Perform the following steps on the column containing the frequency value: remove the AF= part using the gsub function converts the data type of this column to numeric saves only this column into a new variable We now have the frequency value available for filtering and we’ll do that by adding it to the GRanges object we create next: Create a GRanges object as we’ve done for assignment 3 by simply creating an IRanges object where the start and end parameters both get the variant position column (POS column). Also provide the seqnames parameter to GRanges which get the contents of the chromosome column. Note that we do not need to use a for-loop for this. GRanges objects can contain other data as well, called associated metadata. Using the mcols function we can see existing or assign data to each variant. Adding this as a data-frame allows us to set a name for this column as demonstrated below with an example from the previous chapter: # Select and show data for a single gene (note that it specifies &quot;... and 0 metadata columns&quot;) # (&#39;granges&#39; object is used from assignment 3) granges ## GRanges object with 5 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## SOD2 6 160103505-160103690 * ## SOD2 6 160105866-160106085 * ## SOD2 6 160109138-160109294 * ## SOD2 6 160113673-160113915 * ## SOD2 6 160114157-160114219 * ## ------- ## seqinfo: 18 sequences from an unspecified genome; no seqlengths # Generate some random values for &#39;allele frequency&#39; allele_frequency &lt;- round(rnorm(5, mean = 0.45, sd = 0.35), 1) # Bind this data to the &#39;GRanges&#39; object mcols(granges) &lt;- DataFrame(AF=allele_frequency) granges ## GRanges object with 5 ranges and 1 metadata column: ## seqnames ranges strand | AF ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; ## SOD2 6 160103505-160103690 * | 0.2 ## SOD2 6 160105866-160106085 * | 0.9 ## SOD2 6 160109138-160109294 * | 0.6 ## SOD2 6 160113673-160113915 * | 0.7 ## SOD2 6 160114157-160114219 * | 0.2 ## ------- ## seqinfo: 18 sequences from an unspecified genome; no seqlengths Use the example above to associate the frequency value with the created GRanges object. Now that we have both the BED-data (in a GRangesList object) and the VCF data in a GRanges object, we can Get all variants that fall within an exon Use the findOverlaps function and store its output this requires the query and subject parameters to be set to the variants and BED-GrangesList objects respectively Convert the output to a data-frame using the DataFrame function The result from findOverlaps contains two columns of which we are interested in the querytHits column; the rows from the VCF data that lie within an exon. You can use this column to subset the GRanges VCF object with. Note that the subjectHits column describes not the exons but the genes; meaning a subjectHit value of 1 refers to a hit within any of the first gene exons (ABCC9 in case of the cardiopanel). Filter the remaining variants based on their frequency, using a minimum of 30% Instead of removing rows or creating another subset, make sure you know which rows are to be kept after both filtering steps as we are going to reconstruct the original VCF file (we need it in Galaxy) Manually sample a few rows to see if they do fall within an exon and have a frequency &gt; 30% Now that you have all the row numbers of variants that we want to keep, you need to think of a way to re-construct the VCF file and save it to disk. Hint: read in the VCF file again for sub-setting using the readLines function Note: do not forget to include the 17 header lines! Upload the new VCF file into Galaxy as we’ll use it in the next chapter. 4.0.2 Assignment 9; Variant Visualization We will create two simple visualizations: Visualize the allele frequency for all remaining variants using the hist function. Remember that you can use the mcols function to get a metadata column or use the $-sign and the name of the column. Pass the argument breaks=20 to hist and answer the following question in your lab-journal: Can you explain the two peaks that you see in your histogram (around 50% and 100%)? Visualize the amount of variants per gene. A simple solution for getting the numbers is to use the table function on the subjectHits column (output of findOverlaps where subject are the genes). Convert it to a data frame and then use the gene indices to get the actual gene names. Now you should have a combination of gene and number of variants. Create a barplot with this data as in the previous chapter and use cex.names=0.7 to scale the gene names to make them all visible. It is also possible to add the VCF file to IGV (together with the mapping- and BED-files) and visualize the variants in the context of the genes. This is however not a requirement for this assignment. 4.1 Variant Annotation In the - for now - final Galaxy workflow step for analyzing our patient, we are going to annotate all our filtered variants to determine the severity of the variations we determined last time with the LoFreq tool. This involves comparing all our variants against a number of databases containing annotation data for known variants. We will use our filtered VCF-file resulting from LoFreq so we need to upload our newly created VCF file to Galaxy. Make sure that Galaxy recognizes the file type by selecting the correct file type when uploading or change it afterwards using the Edit attributes button. 4.2 SnpEff and SnpSift There are multiple methods of annotating found variants. Most of these will require extra data to predict the effect of a variant or to compare it with a known set of variants. Here we use a set of tools that does both types of annotation. See for further details and a manual the Github documentation. SnpEff: is a variant annotation and effect prediction tool. It annotates and predicts the effects of genetic variants (such as amino acid changes) SnpSift: is a toolbox that allows you to filter and manipulate annotated files. actually, this tool also annotates variants using a variety of data sources, for which we will use it. Both tools are available in Galaxy with numerous sub-tools depending on the task that you want to do. To get the most information for each variant, we will perform three annotation runs: Step 1: Using SnpEff to annotate variants using the GRCh38.86 database adding information related to the gene, transcript and protein that are affected by this variant. This results in a lot of extra data that is all documented in the changed VCF header. For example, given the following variant as input: chr1 237550644 . C A 91.0 PASS DP=578;AF=0.017301;SB=1;DP4=252,316,5,5 This is the result after SnpEff: chr1 237550644 . C A 91.0 PASS DP=578;AF=0.017301;SB=1;DP4=252,316,5,5;ANN=A|missense_variant|MODERATE|RYR2|ENSG00000198626|transcript|ENST00000366574.6|protein_coding|27/105|c.3167C&gt;A|p.Thr1056Lys|3484/16562|3167/14904|1056/4967||,A|missense_variant|MODERATE|RYR2|ENSG00000198626|transcript|ENST00000360064.7|protein_coding|25/103|c.3119C&gt;A|p.Thr1040Lys|3119/14850|3119/14850|1040/4949||WARNING_TRANSCRIPT_NO_START_CODON,A|downstream_gene_variant|MODIFIER|SNORA25|ENSG00000252290|transcript|ENST00000516481.1|snoRNA||n.*4396G&gt;T|||||4396| All details are added in the ANN field. See the description in the VCF header for a field-by-field explanation of the values. Running SnpEff: Select the SnpEff Variant effect and annotation tool from the Variant Calling tool menu in Galaxy. Select the uploaded VCF file containing the variants filtered for the cardiopanel genes. Check if the by default selected GRCh38.86 Genome Source is selected. All other settings can be left default, execute the tool. The result consists of two files: an annotated VCF file and a web-report on all the variants and their predicted effects. Step 2: Adding dbSnp entries using SnpSift. The dbSnp database contains information on known variants. The version that also includes clinical significance (combining the ClinVar database) is available for the SnpSift tool in Galaxy. Running SnpSift for dbsnp: SnpSift requires the dbSnp database to be available in your history: In Galaxy, go to Shared Data (top menu) –&gt; Data Libraries –&gt; dbSnp library Select both files shown there and use the Add to history –&gt; as datasets option to get them in your own history Select the SnpSift Annotate SNPs from dbSnp tool from the Variant Calling tool menu in Galaxy. Note: this is a different tool from the dbNSFP database, see step #3 below. Select the output VCF file created by SnpEff in step #1 as the Variant input file. Select the dbSnp_clinvar.vcf.gz file as the VCF File with ID field annotated. For Fields to annotate select the Add also INFO fields option and execute the tool. This tool adds annotations in two columns. First, the ID column will describe the dbSnp ID (rs number) that can be used to look up the variant. Second, the INFO column is expanded with (a lot of) extra information if the variant is known. For instance, the ClinVar data might report on diseases the variant is related to, such as: CLNDN=Cardiovascular_phenotype|Charcot-Marie-Tooth_disease|Lipoatrophy_with_Diabetes,_Hepatic_Steatosis,_Hypertrophic_Cardiomyopathy,_and_Leukomelanodermic_Papules Step 3: Annotating using the dbNSFP database and SnpSift. The dbNSFP database is a huge collection (over 300GB of text files) of annotation sources combined into a single database. The SnpSift tool can compare our variants with known variants from all of these annotation sources and - like the first few steps - adds them to our VCF file. As you can expect, our VCF file will become very large and most likely unreadable. Therefore, we re-use the VCF file gained from step #1 instead of the VCF file from SnpSift in step #2. We can later combine the data after parsing both files separately. Running SnpSift for dbNSFP: Select the SnpSift dbNSFP tool from the Variant Calling tool menu in Galaxy. Use the VCF file from step #1 as input Select the locally available GRCh38 dbNSFP4.3c Genome database Select all options for the Annotate with option Note that the tool description lists output not available for selection, we will look into the specifics of all selected databases later on. Assignment Create a table in your report where you summarize the added annotation data (very!) briefly; The database where the data comes from The result or value (is it an identifier, a percentage, etc.). Hint: you can add formatted tables in markdown with some syntax which is easiest generated at https://www.tablesgenerator.com The VCF header only describes the names of the databases the data originates from and does not give a clear explanation. There might be a single source for the documentation (please let us know if you find it!), but for now the best method of getting information about the added data is: Step #1: the output of the first step also includes a web report that at least gives more details on the results. Either use your biological knowledge or search for terms if they’re unclear Step #2: data from the dbSnp database comes from the ‘ClinVar’ version of this database, which is documented on its NCBI website. Step #3: for the dbNSFP database there are descriptions available on the online version: Go to the online dbNSFP instance, select the Academic option and click Connect. If you hover over any of the selectable Variant fields, you can read a brief description of the data that is added. Note that this is for dbNSFP version 4.4a and we use version 4.3c (very minor differences) 4.3 Programming Assignment; Analyzing Variant Annotation Given the possibly hundreds of annotated variants with data from multiple data sources, it’s time to do one last filtering on the data to come up with a list of variants of which we can say, with some certainty, that they are related to the condition. As with the VCF-filter tool that we’ve written, here too will we check each line and decide if it is a variant worth reporting on. Whether we want to keep a variant or not depends on the annotation added by SnpEff/SnpSift and it is your task to come up with an approach for combining data from these data sources to rank the annotated variants. 4.3.1 Assignment 10; Data Preparation This programming assignment asks to take another good look at the data, but now in R to see if everything is in a suitable format to eventually sort the data and extract a top-set of variants related to the condition. As you have seen, all data is essentially stored in a single column that contains multiple separators (such as | and ;). You need to think of and implement a good strategy to convert all this information into a workable data format, such as a data frame. There are a few caveats though that can also be seen when looking at the SnpEff report (the ‘stats’ output). For instance, there are two lines in the overview table that describe the amount of variants and the amount of hits in the SnpEff database: Number of variants (before filter) 145 Number of effects 1,332 Which means that on average every variant has about 10 (!) hits of which all data is placed in the INFO column (most notably in the ANN part of the INFO column). This is caused most often by multiple transcripts (alternative splicing) being affected by this variant. This makes it very difficult to parse and therefore this section gives tips and example code to assist in that task. With this much data from three different steps it is too complex to assess in a single run so we will assess annotation data step by step. Step #1 &amp; #2; SnpEff ANN data and SnpSift ClinVar data According to the VCF header, the ANN data contains the values explained in the SnpEff documentation. Some, or all of them could be of interest. There is a Galaxy tool available to extract interesting fields from the added data. Select the SnpSift Extract Fields from a VCF file into a tabular file tool from the tool menu the tool help or this manual can provide more details Select the annotated VCF file as input Provide the names of the columns of interest. For instance, the following could be used as a query for the ‘Fields to extract’: CHROM POS REF ALT ANN[*].EFFECT ANN[*].IMPACT ANN[*].GENE ANN[*].FEATURE ANN[*].FEATUREID ANN[*].BIOTYPE ANN[*].ERRORS CLNDN CLNSIG Add or remove any other column you find interesting to the above query. Enable the ‘One effect per line’ option Run the tool and download the resulting tabular file for analysis in R The resulting file contains as many rows as effects reported by SnpEff (1332 in this example) Note: always include the CHROM POS REF ALT columns which are needed to identify the variant in the original file. Step #3; SnpSift dbNSFP Similar to the previous step, extract the fields of interest using the SnpSift Extract Fields from a VCF file into a tabular file tool. The dbNSFP entries are all prefixed with dbNSFP_, for instance: dbNSFP_MutationAssessor_score and dbNSFP_SIFT_pred. Once done, download this tabular file too for further analysis. 4.3.2 Assignment 11; Finding Variants of Interest Given the knowledge we have on the data stored in the two dataframes, we can now formulate questions for this data to find a set of variants which are most likely linked to the cardiomyopathy condition. This condition cannot be identified by a single variant but is often caused by several variants and the combination not only determines if someone suffers from cardiomyopathy but also how badly their condition is. The physician who diagnoses patients uses the genetic information of the found variants as one of the sources for confirming the condition. However, presenting a list of 100+ variant positions with a list of numbers from some tools is not how that works in practice; we need to present a short, ordered list of variants that can be related to the condition. Next, define your own sorting, filtering, selecting, etc. procedure to filter and order a top-list of variants that you’d present as aid to a diagnoses. For instance, you might have a variant that had a hit in the ClinVar database which directly links it to cardiomyopathy, this should probably be somewhere around the top of your list. Again, look at the output description that you’ve written earlier to come up with combination(s) of values. There is no golden standard for selecting these variants so it is very important to properly document and argument your decisions. As an end product for this step you are expected to include a nice table including at least 10 variants in your report that contains all relevant information for each of them, i.e. the chromomsome, position, reference and variant nucleotides, gene name and all relevant scores. For all of your variants, try to (briefly!) answer the following questions for each variant: Can you find a link between cardiomyopathy and the variant? What is the effect of the variant? Is it validated? Are there scientific studies supporting the evidence? What does the score indicate? What else can you find about the variant? You can use the following online sources among others: http://www.ncbi.nlm.nih.gov/snp/ http://www.ncbi.nlm.nih.gov/variation/tools/1000genomes/ http://www.ncbi.nlm.nih.gov/projects/SNP/ "],["presentation.html", "Week 5 Presentation 5.1 Galaxy Workflow 5.2 Comparative Genomics [OPTIONAL]", " Week 5 Presentation As described in chapter 1, the analysis and the resulting variants needs to be presented. To make this presentation more appealing, we can add some visual elements to the report. In this section, we will discuss a few options for visualizing the data. Besides a graphical representation, the complete table with all variants and - importantly - the reason why there were selected should be included in the presentation. LollipopPlot This visualization can be helpful in cases where many of the top-variants are located within a single gene. The plot shows the position of the variant on the x-axis and a score on the y-axis. The score can be the AF value from the VCF file or any other score that was added through the annotation steps. Further data (‘dimensions’) can be added through changes to: The color of the lollipop (e.g. red for a high score, green for a low score) The size of the lollipop (e.g. the size of the lollipop is proportional to the score) The symbol of the lollipop (e.g. a star for a variant that is validated) A value displayed within each node Text labels attached to the nodes There are multiple options for drawing these types of graphs but here we demonstrate the use of the lolliplot function from the trackViewer package in R, see the online manual for many examples on how to create this plot. This package is not installed by default in, so you have to install it yourself: BiocManager::install(&quot;trackViewer&quot;) library(trackViewer) It requires two data sources for our use case: The sample data which should be a GRanges object containing the variants the example below shows how we use the output of findOverlaps to get the variants for a single gene The feature data containing the gene on which these variants are located, also in GRanges format to get all exons for the selected gene we use our BED-data which is already in an GRanges object. # Preparing the data; first attach the gene names to the findOverlaps output overlap_df$Gene &lt;- names(bed_data)[overlap_df$subjectHits] # Then, use this to get all variants from a single gene (&#39;RET&#39; in this example) # Note that the vcf_Granges already containes the Allele frequency values (AF) sample.gr &lt;- vcf_GRanges[unlist(subset(overlap_df, Gene == &quot;RET&quot;, select = queryHits))] # The features are taken from the GRangesList object from chapter 3, assignment 3 features &lt;- bed_data$RET This basic example shows 9 variants for the RET gene, with the AF value as the score (scaled to a range of 0-100%). The plot shows the position of the variant on the x-axis and the allele-frequency on the y-axis. Note: scores are a bit weird in this plot; for instance you cannot show values ranging from 0 to 1 (such as SIFT scores). Values between 1-10 are shown as stacked nodes (see the Change the height of a lollipop plot section of the manual). A safe solution is to scale values from 0-100 as shown below. Alternatively, we can use the stacks to show the amount of splice variants at a single location (up to a maximum of 10). # Calculate AF as percentage and set as &#39;score&#39; field sample.gr$score &lt;- sample.gr$AF * 100 head(sample.gr) ## GRanges object with 6 ranges and 2 metadata columns: ## seqnames ranges strand | score AF ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; &lt;numeric&gt; ## [1] chr10 43077059 * | 92.8571 0.928571 ## [2] chr10 43077063 * | 60.0000 0.600000 ## [3] chr10 43100520 * | 99.8780 0.998780 ## [4] chr10 43111239 * | 100.0000 1.000000 ## [5] chr10 43118395 * | 44.8733 0.448733 ## [6] chr10 43119646 * | 45.9846 0.459846 ## ------- ## seqinfo: 21 sequences from an unspecified genome; no seqlengths # RET gene exons head(features) ## GRanges object with 6 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## RET chr10 43077026-43077331 * ## RET chr10 43100458-43100722 * ## RET chr10 43102341-43102629 * ## RET chr10 43104731-43105193 * ## RET chr10 43106375-43106571 * ## RET chr10 43109030-43109230 * ## ------- ## seqinfo: 22 sequences from an unspecified genome; no seqlengths lolliplot(sample.gr, features, ylab = &quot;Allele Frequency (%)&quot;) Simple improvements are to increase the height and color of the exons (all exons share the same value): features$height &lt;- 0.03 features$fill &lt;- &quot;lightblue&quot; head(features) lolliplot(sample.gr, features, ylab = &quot;Allele Frequency (%)&quot;) Adding labels to the variants, in this case some random dbsnp IDs but these could be any other value available: # Generate random dbsnp IDs dbsnp_ids &lt;- paste0(&quot;rs&quot;, sample(239283:532801, length(sample.gr))) # Add names to the IRanges object within the GRanges sample.gr &lt;- setNames(sample.gr, dbsnp_ids) lolliplot(sample.gr, features, ylab = &quot;Allele Frequency (%)&quot;) Changing the color based on the effect impact from the SnpEff Galaxy tool effect &lt;- c(&quot;MODERATE&quot;, &quot;LOW&quot;, &quot;MODIFIER&quot;, &quot;HIGH&quot;) effect.color &lt;- c(&quot;orange&quot;, &quot;blue&quot;,&quot;green&quot;, &quot;red&quot;) # Randomly select an effect for each variant sample.gr$effect &lt;- effect[sample(1:4, length(sample.gr), replace = TRUE)] # Given the effect, get the matching color sample.gr$color &lt;- effect.color[match(sample.gr$effect, effect)] lolliplot(sample.gr, features, ylab = &quot;Allele Frequency (%)&quot;) Dandelion Plot If there are a lot of variants close together, an alternative can be the dandelion plot which is a variation of the lollipop plot where the variants are grouped together based on their position. It’s available from the same library, see the online manual for more information. 5.1 Galaxy Workflow Now that we have completed our analysis steps we can create a Galaxy workflow. In this workflow we will combine all the tools that we have used in Galaxy (FastQ files –&gt; SnpEff/SnpSift annotated output) so that when we receive new patient data in the future we can analyze that dataset with a single click! There are many online resources giving instructions and examples for creating Galaxy workflows, use these resources to create a workflow including all used analysis tools. Note that you can convert Galaxy histories into workflow(s), which saves a lot of time with configuring all the tools from that workfow. Once you have a workflow, make sure it is shared with your project partner and create a screenshot of it for your report (try to fit all tools in a single image). Note; this assignment has a very low priority and should be done once you are satisfied with all other parts of your work! At the bare minimum, open your last history (including SnpEff/SnpSift) and explore/ play with the Extract Workflow option. 5.2 Comparative Genomics [OPTIONAL] With the complete analysis nicely packaged into a single workflow, one thing that we can use it for is to do comparative genomics. This term contains many different types of comparisons and is described on Wikipedia as Comparative genomics is a field of biological research in which the genomic features of different organisms are compared. These features can be of many types and in our case they are the variants. A simple tool is available in Galaxy called VCF-VCFintersect that performs either intersection or union on two VCF datasets containing variant data. This tool compares two VCF files and retains either the shared variants (intersection) or all variants combined (union) as shown in the diagram below. Optional Assignment Process another patient sample (use a new history) by running your complete workflow. Note that you have to filter the resulting SnpEff output (which contains the same data as the VCF file appended with annotation data) with the BED-data. Alternatively, you can stop after the LoFreq tool, download the VCF file, filter this using the BED-data and re-upload the filtered VCF file before continuing with SnpEff Calculate the following three statistics: The number of variants unique to patient 1 The number of variants unique to patient 2 The number of variants found in both patients Create a VENN-diagram (like the image above) displaying these numbers. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
